{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import itertools\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import torch\n",
    "import timm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from torch.amp import autocast, GradScaler\n",
    "import math\n",
    "from torch.nn import DataParallel\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import torch.distributed as dist\n",
    "import argparse\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import psutil\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, device):\n",
    "        # Model Architecture\n",
    "        self.model_name = 'resnet50'\n",
    "        self.image_embedding = 2048\n",
    "        self.text_encoder_model = \"distilbert-base-uncased\"\n",
    "        self.text_embedding = 768\n",
    "        self.text_tokenizer = \"distilbert-base-uncased\"\n",
    "        self.projection_dim = 256\n",
    "        self.dropout = 0.1\n",
    "        \n",
    "        # Training Parameters\n",
    "        self.batch_size = 16  # Reduced batch size\n",
    "        self.num_workers = 2  # Reduced workers\n",
    "        self.head_lr = 1e-3\n",
    "        self.image_encoder_lr = 1e-4\n",
    "        self.text_encoder_lr = 1e-5\n",
    "        self.weight_decay = 1e-3\n",
    "        self.patience = 5\n",
    "        self.factor = 0.5\n",
    "        self.epochs = 15\n",
    "        self.temperature = 1.0\n",
    "        self.max_length = 128  # Reduced sequence length\n",
    "        self.gradient_accumulation_steps = 4  # Increased for smaller batches\n",
    "        \n",
    "        # Image Processing\n",
    "        self.size = 160  # Reduced image size\n",
    "        self.pretrained = True\n",
    "        self.trainable = True\n",
    "        \n",
    "        # Memory Optimization\n",
    "        self.gradient_checkpointing = True\n",
    "        self.mixed_precision = True\n",
    "        self.dataset_shard_size = 5000\n",
    "        self.memory_efficient_loading = True\n",
    "        \n",
    "        # Distributed Training\n",
    "        self.distributed = False\n",
    "        self.world_size = 1\n",
    "        self.rank = 0\n",
    "        \n",
    "        # Device\n",
    "        self.device = device\n",
    "        \n",
    "        # Paths\n",
    "        self.model_save_path = Path(\"models\")\n",
    "        self.log_dir = Path(\"logs\")\n",
    "        self.cache_dir = Path(\"cache\")\n",
    "        \n",
    "        # Create necessary directories\n",
    "        self.model_save_path.mkdir(exist_ok=True)\n",
    "        self.log_dir.mkdir(exist_ok=True)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "class MemoryEfficientDataset(Dataset):\n",
    "    def __init__(self, image_filenames, captions, tokenizer, transforms, image_path, cfg, shard_idx=None):\n",
    "        self.cfg = cfg\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = list(captions)\n",
    "        self.image_path = Path(image_path)\n",
    "        self.transforms = transforms\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Implement sharding if specified\n",
    "        if shard_idx is not None and cfg.dataset_shard_size > 0:\n",
    "            start_idx = shard_idx * cfg.dataset_shard_size\n",
    "            end_idx = min(start_idx + cfg.dataset_shard_size, len(image_filenames))\n",
    "            self.image_filenames = image_filenames[start_idx:end_idx]\n",
    "            self.captions = captions[start_idx:end_idx]\n",
    "        \n",
    "        # Validate images and create index mapping\n",
    "        self._validate_and_create_mapping()\n",
    "        \n",
    "        # Tokenize captions efficiently\n",
    "        self._tokenize_captions()\n",
    "\n",
    "    def _validate_and_create_mapping(self):\n",
    "        logging.info(\"Validating images and creating mapping...\")\n",
    "        self.valid_indices = []\n",
    "        self.file_sizes = {}\n",
    "        \n",
    "        for idx, fname in enumerate(tqdm(self.image_filenames)):\n",
    "            img_path = self.image_path / fname\n",
    "            if self._validate_image(img_path):\n",
    "                self.valid_indices.append(idx)\n",
    "                self.file_sizes[idx] = img_path.stat().st_size\n",
    "        \n",
    "        logging.info(f\"Found {len(self.valid_indices)} valid images\")\n",
    "        self._filter_valid_data()\n",
    "\n",
    "    def _validate_image(self, path):\n",
    "        try:\n",
    "            return path.exists() and path.stat().st_size > 0\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def _filter_valid_data(self):\n",
    "        self.filtered_filenames = [self.image_filenames[i] for i in self.valid_indices]\n",
    "        self.filtered_captions = [self.captions[i] for i in self.valid_indices]\n",
    "\n",
    "    def _tokenize_captions(self):\n",
    "        logging.info(\"Tokenizing captions...\")\n",
    "        self.encoded_captions = self.tokenizer(\n",
    "            self.filtered_captions,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.cfg.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def _load_image(self, img_path):\n",
    "        try:\n",
    "            image = cv2.imread(str(img_path))\n",
    "            if image is None:\n",
    "                raise ValueError(\"Image could not be loaded\")\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            item = {\n",
    "                key: self.encoded_captions[key][idx]\n",
    "                for key in self.encoded_captions\n",
    "            }\n",
    "            \n",
    "            img_path = self.image_path / self.filtered_filenames[idx]\n",
    "            image = self._load_image(img_path)\n",
    "            \n",
    "            if image is None:\n",
    "                # Return a different valid image if this one fails\n",
    "                return self.__getitem__((idx + 1) % len(self))\n",
    "            \n",
    "            image = self.transforms(image=image)['image']\n",
    "            item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "            item['caption'] = self.filtered_captions[idx]\n",
    "            \n",
    "            return item\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing index {idx}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_filenames)\n",
    "\n",
    "class MemoryEfficientImageEncoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            num_classes=0,\n",
    "            global_pool=\"avg\"\n",
    "        )\n",
    "        \n",
    "        # Enable gradient checkpointing if configured\n",
    "        if cfg.gradient_checkpointing:\n",
    "            self.model.set_grad_checkpointing(enable=True)\n",
    "        \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = cfg.trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class MemoryEfficientTextEncoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.model = DistilBertModel.from_pretrained(cfg.text_encoder_model)\n",
    "        \n",
    "        # Enable gradient checkpointing if configured\n",
    "        if cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = cfg.trainable\n",
    "        \n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, cfg, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, cfg.projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(cfg.projection_dim, cfg.projection_dim)\n",
    "        self.dropout = nn.Dropout(cfg.dropout)\n",
    "        self.layer_norm = nn.LayerNorm(cfg.projection_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class OptimizedCLIPModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.image_encoder = MemoryEfficientImageEncoder(cfg)\n",
    "        self.text_encoder = MemoryEfficientTextEncoder(cfg)\n",
    "        self.image_projection = ProjectionHead(cfg, cfg.image_embedding)\n",
    "        self.text_projection = ProjectionHead(cfg, cfg.text_embedding)\n",
    "        self.temperature = cfg.temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        def compute_image_features():\n",
    "            return self.image_encoder(batch[\"image\"])\n",
    "            \n",
    "        def compute_text_features():\n",
    "            return self.text_encoder(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"]\n",
    "            )\n",
    "        \n",
    "        # Use checkpointing if enabled\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            image_features = checkpoint(compute_image_features)\n",
    "            text_features = checkpoint(compute_text_features)\n",
    "        else:\n",
    "            image_features = compute_image_features()\n",
    "            text_features = compute_text_features()\n",
    "        \n",
    "        # Project features\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "        \n",
    "        # Calculate similarity and loss\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, \n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss = (images_loss + texts_loss) / 2.0\n",
    "        return loss.mean()\n",
    "\n",
    "class MemoryTracker:\n",
    "    @staticmethod\n",
    "    def get_memory_usage():\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "            gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**2  # MB\n",
    "        else:\n",
    "            gpu_memory = gpu_memory_reserved = 0\n",
    "            \n",
    "        ram_memory = psutil.Process().memory_info().rss / 1024**2  # MB\n",
    "        return {\n",
    "            'gpu_allocated': gpu_memory,\n",
    "            'gpu_reserved': gpu_memory_reserved,\n",
    "            'ram': ram_memory\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def log_memory_usage(stage):\n",
    "        memory = MemoryTracker.get_memory_usage()\n",
    "        logging.info(f\"Memory usage at {stage}:\")\n",
    "        logging.info(f\"  GPU Memory Allocated: {memory['gpu_allocated']:.2f} MB\")\n",
    "        logging.info(f\"  GPU Memory Reserved: {memory['gpu_reserved']:.2f} MB\")\n",
    "        logging.info(f\"  RAM Usage: {memory['ram']:.2f} MB\")\n",
    "\n",
    "def get_transforms(mode=\"train\", size=224):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose([\n",
    "            A.Resize(size, size, always_apply=True),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(size, size, always_apply=True),\n",
    "            A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "        ])\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()\n",
    "\n",
    "def build_loaders(dataframe, tokenizer, cfg, image_path, mode=\"train\"):\n",
    "    transforms = get_transforms(mode=mode, size=cfg.size)\n",
    "    \n",
    "    # Calculate number of shards\n",
    "    num_samples = len(dataframe)\n",
    "    num_shards = math.ceil(num_samples / cfg.dataset_shard_size)\n",
    "    \n",
    "    # Create datasets for each shard\n",
    "    datasets = []\n",
    "    for shard_idx in range(num_shards):\n",
    "        dataset = MemoryEfficientDataset(\n",
    "            dataframe[\"image\"].values,\n",
    "            dataframe[\"caption\"].values,\n",
    "            tokenizer=tokenizer,\n",
    "            transforms=transforms,\n",
    "            image_path=image_path,\n",
    "            cfg=cfg,\n",
    "            shard_idx=shard_idx\n",
    "        )\n",
    "        datasets.append(dataset)\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "    \n",
    "    # Create sampler for distributed training\n",
    "    sampler = DistributedSampler(combined_dataset) if cfg.distributed else None\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        combined_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        num_workers=cfg.num_workers,\n",
    "        shuffle=(mode == \"train\" and not cfg.distributed),\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        sampler=sampler\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "    \n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>cloudcover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20160101075000.jpg</td>\n",
       "      <td>Image has No Cloud Coverage. Image has 7% of o...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20160101080000.jpg</td>\n",
       "      <td>Image has No Cloud Coverage. Image has 7% of o...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20160101081000.jpg</td>\n",
       "      <td>Image has No Cloud Coverage. Image has 7% of o...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20160101082000.jpg</td>\n",
       "      <td>Image has No Cloud Coverage. Image has 7% of o...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20160101083000.jpg</td>\n",
       "      <td>Image has No Cloud Coverage. Image has 7% of o...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image                                            caption  \\\n",
       "0  20160101075000.jpg  Image has No Cloud Coverage. Image has 7% of o...   \n",
       "1  20160101080000.jpg  Image has No Cloud Coverage. Image has 7% of o...   \n",
       "2  20160101081000.jpg  Image has No Cloud Coverage. Image has 7% of o...   \n",
       "3  20160101082000.jpg  Image has No Cloud Coverage. Image has 7% of o...   \n",
       "4  20160101083000.jpg  Image has No Cloud Coverage. Image has 7% of o...   \n",
       "\n",
       "   cloudcover  \n",
       "0           7  \n",
       "1           7  \n",
       "2           7  \n",
       "3           7  \n",
       "4           7  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = 'dataset'\n",
    "df = pd.read_csv(folder+\"/x.csv\")\n",
    "df = df[['image_name', 'label', 'opaque_clouds']]\n",
    "df.columns = ['image', 'caption', 'cloudcover']\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((29304,), (12560,), (13955,))\n"
     ]
    }
   ],
   "source": [
    "x = df['image']\n",
    "y = df['cloudcover']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=48)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=48)\n",
    "\n",
    "print((x_train.shape, x_val.shape, x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 14:05:46,092 - INFO - Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "2024-10-27 14:05:46,570 - INFO - [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedCLIPModel(\n",
       "  (image_encoder): MemoryEfficientImageEncoder(\n",
       "    (model): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_block): Identity()\n",
       "          (act2): ReLU(inplace=True)\n",
       "          (aa): Identity()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "      (fc): Identity()\n",
       "    )\n",
       "  )\n",
       "  (text_encoder): MemoryEfficientTextEncoder(\n",
       "    (model): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0-5): 6 x TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (image_projection): ProjectionHead(\n",
       "    (projection): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_projection): ProjectionHead(\n",
       "    (projection): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG = Config(\"cuda\")\n",
    "model = OptimizedCLIPModel(CFG)\n",
    "\n",
    "checkpoint = torch.load(\"models/best_model.pt\", map_location=CFG.device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Custom Dataset Loader ----- #\n",
    "img_folder = \"dataset/data_images/Extracted Images/\"\n",
    "# ----- Custom Dataset Loader ----- #\n",
    "class SkyImage(Dataset):\n",
    "\tdef __init__(self, img_dir, labels):\n",
    "\t\tself.img_dir = img_dir\n",
    "\t\tself.img_labels = labels\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.img_dir)\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg_path = os.path.join(img_folder, self.img_dir[idx])\n",
    "\t\t#os.path.join(\"Extracted Images/\", self.img_dir[idx])\n",
    "\t\timage = cv2.imread(img_path)\n",
    "\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\t\timage = cv2.resize(image, (244, 244))\n",
    "\t\timage = np.moveaxis(image, -1, 0)\n",
    "\t\tlabel = self.img_labels[idx]\n",
    "\t\treturn image, label\n",
    "\n",
    "# ----- Dataset ----- #\n",
    "train_images = SkyImage(x_train.to_list(), y_train.to_list())\n",
    "valid_images = SkyImage(x_val.to_list(), y_val.to_list())\n",
    "test_images = SkyImage(x_test.to_list(), y_test.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151498e797564a8d8b9d6d3f0da6e57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/458 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c9376911fb4c0ebd510dcdb76d58ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe21c2f7d5a47a9a877a968da5eaed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# First move the model to the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "def get_features(dataset):\n",
    "    all_features, all_labels, all_embeddings = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=64)):\n",
    "            # Move labels to device immediately when loading\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            image_input = torch.tensor(np.stack(images), device=device, dtype=torch.float32)\n",
    "            \n",
    "            image_features = model.image_encoder(image_input)\n",
    "            image_embeddings = model.image_projection(image_features)\n",
    "            \n",
    "            all_features.append(image_features)\n",
    "            all_labels.append(labels)\n",
    "            all_embeddings.append(image_embeddings)\n",
    "    \n",
    "    return (\n",
    "        torch.cat(all_features),\n",
    "        torch.cat(all_labels),\n",
    "        torch.cat(all_embeddings)\n",
    "    )\n",
    "\n",
    "# Get features for each dataset\n",
    "train_features, train_labels, train_embeddings = get_features(train_images)\n",
    "valid_features, valid_labels, valid_embeddings = get_features(valid_images)\n",
    "test_features, test_labels, test_embeddings = get_features(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "def evaluate(name, x, y, n, p): # p: features, #n: no of observations\n",
    "\tprint(\"---------------------------------------------------\")\n",
    "\tprint(\"{} MAE: {}\".format(name, mean_absolute_error(x, y)))\n",
    "\tprint(\"{} RMSE: {}\".format(name, mean_squared_error(x, y, squared=False)))\n",
    "\tprint(\"{} MSE: {}\".format(name, mean_squared_error(x, y)))\n",
    "\tr2 = r2_score(x, y)\n",
    "\tprint(\"{} R2: {}\".format(name, r2))\n",
    "\tprint(\"---------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA (GPU) is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9bfb025f82418ca84e8b250ebb1c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 26.5837347\ttest: 26.5585471\tbest: 26.5585471 (0)\ttotal: 570ms\tremaining: 6m 38s\n",
      "50:\tlearn: 10.5618414\ttest: 11.1900676\tbest: 11.1900676 (50)\ttotal: 14.1s\tremaining: 2m 59s\n",
      "100:\tlearn: 9.6116118\ttest: 10.7145166\tbest: 10.7145166 (100)\ttotal: 26.4s\tremaining: 2m 36s\n",
      "150:\tlearn: 8.7295100\ttest: 10.4132389\tbest: 10.4132389 (150)\ttotal: 39s\tremaining: 2m 21s\n",
      "200:\tlearn: 8.0439690\ttest: 10.2155464\tbest: 10.2155464 (200)\ttotal: 51.2s\tremaining: 2m 7s\n",
      "250:\tlearn: 7.4913155\ttest: 10.0907563\tbest: 10.0907563 (250)\ttotal: 1m 4s\tremaining: 1m 55s\n",
      "300:\tlearn: 7.0292616\ttest: 9.9941724\tbest: 9.9941724 (300)\ttotal: 1m 17s\tremaining: 1m 42s\n",
      "350:\tlearn: 6.6008116\ttest: 9.9091332\tbest: 9.9091332 (350)\ttotal: 1m 29s\tremaining: 1m 29s\n",
      "400:\tlearn: 6.2431446\ttest: 9.8518879\tbest: 9.8518879 (400)\ttotal: 1m 41s\tremaining: 1m 15s\n",
      "450:\tlearn: 5.8764856\ttest: 9.7898503\tbest: 9.7898503 (450)\ttotal: 1m 54s\tremaining: 1m 2s\n",
      "500:\tlearn: 5.5420748\ttest: 9.7367897\tbest: 9.7367897 (500)\ttotal: 2m 6s\tremaining: 50.1s\n",
      "550:\tlearn: 5.2570667\ttest: 9.7022205\tbest: 9.7015876 (548)\ttotal: 2m 18s\tremaining: 37.4s\n",
      "600:\tlearn: 4.9862659\ttest: 9.6765326\tbest: 9.6765326 (600)\ttotal: 2m 30s\tremaining: 24.7s\n",
      "650:\tlearn: 4.7359421\ttest: 9.6444863\tbest: 9.6444863 (650)\ttotal: 2m 42s\tremaining: 12.2s\n",
      "699:\tlearn: 4.5190845\ttest: 9.6249126\tbest: 9.6249126 (699)\ttotal: 2m 53s\tremaining: 0us\n",
      "\n",
      "bestTest = 9.624912586\n",
      "bestIteration = 699\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x25383f37140>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- Model Training ----- #\n",
    "\n",
    "CB_model = CatBoostRegressor(iterations=700, learning_rate=0.1, max_depth=8, eval_metric='RMSE', random_seed=48)\n",
    "CB_model.fit(train_features.cpu().numpy(), train_labels.cpu().numpy(),\n",
    "\t\t\teval_set = (valid_features.cpu().numpy(), valid_labels.cpu().numpy()),\n",
    "\t\t\tuse_best_model=True, plot=True, verbose=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Model Prediction ----- #\n",
    "\n",
    "cbt_train_pred = CB_model.predict(train_features.cpu().numpy())\n",
    "cbt_valid_pred = CB_model.predict(valid_features.cpu().numpy())\n",
    "cbt_test_pred = CB_model.predict(test_features.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "Train MAE: 3.063867080191343\n",
      "Train RMSE: 4.519084504196453\n",
      "Train MSE: 20.422124756068502\n",
      "Train R2: 0.9753638347539638\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "Valid MAE: 5.798734045374874\n",
      "Valid RMSE: 9.624912586359335\n",
      "Valid MSE: 92.63894229505833\n",
      "Valid R2: 0.8879491192051271\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "Test MAE: 5.812910249786526\n",
      "Test RMSE: 9.69202086124033\n",
      "Test MSE: 93.93526837471777\n",
      "Test R2: 0.886006145423602\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ----- Model Evaluation ----- #\n",
    "\n",
    "evaluate(\"Train\", train_labels.cpu(), cbt_train_pred, len(cbt_train_pred), 1)\n",
    "evaluate(\"Valid\", valid_labels.cpu(), cbt_valid_pred, len(cbt_valid_pred), 1)\n",
    "evaluate(\"Test\", test_labels.cpu(), cbt_test_pred, len(cbt_test_pred), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(CB_model, open('catboost_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
